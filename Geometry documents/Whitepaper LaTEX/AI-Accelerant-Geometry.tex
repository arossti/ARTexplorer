\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{parskip}  % Adds vertical space between paragraphs
\usepackage{tcolorbox}  % For framed boxes
\usepackage{endnotes}   % For endnotes instead of footnotes

\geometry{margin=1in}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=blue,
    citecolor=blue
}

\newtheorem{conjecture}{Conjecture}
\newtheorem{definition}{Definition}
\newtheorem{observation}{Observation}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\title{Algebraic Geometry as AI Accelerant:\\
\large Quadray Coordinates and Rational Trigonometry\\
for Hardware-Efficient Neural Computation}

\author{Andrew Thomson\\
\small Open Building / ARTexplorer Project\\
\small \href{mailto:andy@openbuilding.ca}{andy@openbuilding.ca}}

\date{February 2026 -- Draft v0.1}

\begin{document}

\maketitle

\begin{abstract}
Recent work by Zhang~(2025) demonstrates that Grassmann manifold geometry can replace attention mechanisms in transformer architectures, achieving competitive performance with linear scaling in sequence length. We argue that this result is an instance of a deeper principle: \textbf{the right algebraic geometry eliminates unnecessary computation}. We propose that Quadray coordinates (tetrahedral $\mathbb{R}^4$ basis) combined with Wildberger's Rational Trigonometry (spread/cross algebra) provide a concrete, hardware-friendly algebraic toolkit for geometric AI---one that produces \textbf{exact rational outputs from rational inputs}, maps naturally to integer/fixed-point arithmetic, and exploits circulant matrix structure for $O(n \log n)$ transforms.

Where Zhang proves geometry can replace attention, we describe the specific algebra that could make geometric AI \emph{cheap}: no transcendental functions at the core computation layer, deferred radical expansion, quantization-friendly exactness, and a native 4-dimensional structure that matches multi-head attention topology.
\end{abstract}

\tableofcontents
\newpage

%==============================================================================
\section{Introduction: The Geometric Turn in AI}
%==============================================================================

\subsection{From Attention to Geometry}

The transformer architecture~(Vaswani et al., 2017) computes attention scores via:
\begin{equation}\label{eq:attention}
\text{Attention}(Q, K, V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\end{equation}

Every element of the softmax requires evaluating $\exp(\cdot)$---a transcendental function that:
\begin{itemize}
    \item Requires Taylor series approximation on hardware (typically 10--20 FLOPs per evaluation)
    \item Produces irrational outputs from rational inputs
    \item Accumulates floating-point error through numerical overflow/underflow corrections
    \item Is hostile to quantization (INT8/INT4 inference)
\end{itemize}

Zhang~(2025) showed that replacing attention with geometric operations on Grassmann manifolds---specifically, encoding token pairs as 2-dimensional subspaces via Pl\"ucker coordinates---achieves competitive language modelling performance with \textbf{linear scaling} in sequence length. The core computation becomes controlled deformation of low-rank subspaces rather than exponentiation over unstructured tensor space.

This paper was not isolated. The ``geometric turn'' in AI reflects a growing recognition that the mathematical structures underlying neural computation---rotations, projections, subspace relationships---are fundamentally \emph{geometric}, and that working with native geometric representations eliminates computational overhead introduced by coordinate-dependent encodings.

\subsection{The Parallel Discovery}

The ARTexplorer project arrived at a structurally identical principle from a different tradition: combining R.\ Buckminster Fuller's Quadray coordinate system (tetrahedral $\mathbb{R}^4$ basis) with Norman J.\ Wildberger's Rational Trigonometry (spread/cross algebra). The resulting system---Spread-Quadray Rotors---achieves:

\begin{itemize}
    \item \textbf{Exact rational arithmetic} for rotations at algebraically significant angles
    \item \textbf{No transcendental functions} at the core computation layer
    \item \textbf{Deferred radical expansion}---$\sqrt{\cdot}$ evaluated once at the hardware boundary
    \item \textbf{Circulant matrix structure} enabling $O(n \log n)$ transforms via FFT
    \item \textbf{Gimbal-lock-free} rotation via topological lift to $\mathbb{R}^4 \times \mathbb{Z}_2$
\end{itemize}

This paper argues that the convergence is not coincidental. The same algebraic structures that make geometric AI \emph{possible} (Zhang) can be made \emph{hardware-efficient} through rational trigonometric algebra (Wildberger) in tetrahedral coordinates (Fuller/Urner).

\subsection{Thesis}

\begin{tcolorbox}[colback=blue!5!white, colframe=blue!50!black, title=Central Claim]
Rational Trigonometry in Quadray coordinates is the natural algebra for quantized geometric AI. It replaces transcendental functions with polynomial/rational operations, produces exact outputs amenable to integer arithmetic, and exploits tetrahedral symmetry for structured sparsity---yielding faster inference at lower precision with higher accuracy.
\end{tcolorbox}

%==============================================================================
\section{Background: Two Traditions Converge}
%==============================================================================

\subsection{Grassmann Geometry in AI (Zhang, 2025)}

Zhang's ``Attention Is Not What You Need'' proposes \textbf{Causal Grassmann layers} as a replacement for self-attention. The architecture:

\begin{enumerate}
    \item \textbf{Linear reduction}: Token hidden states are projected to lower dimension
    \item \textbf{Grassmann encoding}: Local token pairs are encoded as 2-dimensional subspaces on the Grassmann manifold $\text{Gr}(2, d)$ via Pl\"ucker coordinates
    \item \textbf{Gated fusion}: Information is mixed back into hidden states through algebraic (not transcendental) gating operations
\end{enumerate}

\textbf{Key results} (13--18M parameter models):
\begin{itemize}
    \item Wikitext-2 perplexity within 10--15\% of transformer equivalents
    \item SNLI validation accuracy 0.8550 vs.\ 0.8545 for standard attention (slight improvement)
    \item Linear scaling in sequence length for fixed rank (vs.\ quadratic for attention)
\end{itemize}

The critical insight: ``Information propagates by controlled deformations of low-rank subspaces over multi-scale local windows, so the core computation lives on a finite-dimensional manifold rather than in an unstructured tensor space.''

\subsection{Rational Trigonometry (Wildberger, 2005)}

Wildberger's Rational Trigonometry replaces the classical distance/angle framework with:

\begin{definition}[Quadrance]
$Q(P_1, P_2) = (x_2 - x_1)^2 + (y_2 - y_1)^2 + (z_2 - z_1)^2$
\end{definition}

\begin{definition}[Spread]
$s(\mathbf{v}_1, \mathbf{v}_2) = 1 - \frac{(\mathbf{v}_1 \cdot \mathbf{v}_2)^2}{Q(\mathbf{v}_1) \cdot Q(\mathbf{v}_2)}$
\end{definition}

Spread measures ``perpendicularity'' between vectors: $s = 0$ for parallel, $s = 1$ for perpendicular. Crucially, $s = \sin^2\theta$, so spread is the \textbf{square} of the classical sine---and squaring collapses transcendental values to rational ones at algebraically significant angles:

\begin{center}
\begin{tabular}{cccc}
\toprule
\textbf{Angle} & $\sin\theta$ & $\cos\theta$ & \textbf{Spread} $s = \sin^2\theta$ \\
\midrule
$0°$ & $0$ & $1$ & $\mathbf{0}$ \\
$30°$ & $1/2$ & $\sqrt{3}/2$ & $\mathbf{1/4}$ \\
$45°$ & $\sqrt{2}/2$ & $\sqrt{2}/2$ & $\mathbf{1/2}$ \\
$60°$ & $\sqrt{3}/2$ & $1/2$ & $\mathbf{3/4}$ \\
$90°$ & $1$ & $0$ & $\mathbf{1}$ \\
$120°$ & $\sqrt{3}/2$ & $-1/2$ & $\mathbf{3/4}$ \\
$180°$ & $0$ & $-1$ & $\mathbf{0}$ \\
\bottomrule
\end{tabular}
\end{center}

All spreads are \textbf{exact rationals}, even when $\sin\theta$ and $\cos\theta$ are irrational. This is the foundation of the computational advantage.

\subsection{Quadray Coordinates (Fuller/Urner)}

The Quadray coordinate system uses four basis vectors directed toward the vertices of a regular tetrahedron:
\begin{align}
\hat{W} &= (1, 0, 0, 0) \\
\hat{X} &= (0, 1, 0, 0) \\
\hat{Y} &= (0, 0, 1, 0) \\
\hat{Z} &= (0, 0, 0, 1)
\end{align}

In Cartesian space, these same vectors require $\sqrt{3}$:
\begin{equation}
\hat{W}_{\text{xyz}} = \tfrac{1}{\sqrt{3}}(1, 1, 1), \quad
\hat{X}_{\text{xyz}} = \tfrac{1}{\sqrt{3}}(1, -1, -1), \quad \text{etc.}
\end{equation}

The mutual spread between any two Quadray basis vectors is:
\begin{equation}
s(\hat{W}, \hat{X}) = \sin^2(109.47°) = \frac{8}{9} \quad \text{(exact rational!)}
\end{equation}

The tetrahedral central angle $\cos^{-1}(-1/3) \approx 109.47°$ is the natural angular quantum of this coordinate system, and its spread $8/9$ is exactly representable in any fixed-point format.

%==============================================================================
\section{Structural Correspondence: Grassmann Layers $\leftrightarrow$ Spread-Quadray Algebra}
%==============================================================================

The correspondence between Zhang's geometric AI and Quadray-RT is not metaphorical---it is structural. Both systems execute the same three-phase computational pattern:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{Phase} & \textbf{Zhang (Grassmann)} & \textbf{Quadray-RT (Spread-Rotor)} \\
\midrule
\textbf{1. Lift} & Tokens $\to$ Grassmann manifold & $\mathbb{R}^3 \to \mathbb{R}^4 \times \mathbb{Z}_2$ \\
\textbf{2. Transform} & Subspace deformation (algebraic) & F,G,H rotation (polynomial) \\
\textbf{3. Project} & Grassmann $\to$ hidden states & Rotor $\to$ Matrix3 at GPU boundary \\
\bottomrule
\end{tabular}
\end{center}

\subsection{Phase 1: Dimensional Lifting to Escape Singularities}

Zhang lifts from tensor space to the Grassmann manifold to escape quadratic scaling---a computational ``singularity'' of standard attention. Quadray-RT lifts from $\mathbb{R}^3$ to $\mathbb{R}^4 \times \mathbb{Z}_2$ to escape gimbal lock---a topological singularity of $SO(3)$.

The underlying principle is identical: the \textbf{Hairy Ball Theorem} (Brouwer, 1912) guarantees that any continuous parameterization of an even-dimensional sphere must have singularities. Both systems escape by adding a dimension.

\begin{theorem}[Topological Obstruction --- applies to both domains]
You cannot smoothly cover the full space of transformations with fewer parameters than the topology demands without creating singularities. In rotation: gimbal lock. In attention: quadratic blowup. The solution in both cases: lift to a higher-dimensional space where the topology is simply connected.
\end{theorem}

\subsection{Phase 2: Algebraic Core Computation}

In the lifted space, both systems perform their core computation using \textbf{algebraic} (polynomial/rational) operations rather than transcendental ones:

\begin{itemize}
    \item \textbf{Zhang}: Pl\"ucker coordinates are quadratic functions of the input vectors. Subspace deformation is linear algebra on these coordinates.
    \item \textbf{Quadray-RT}: Spread is a rational function of dot products. Rotation via F,G,H coefficients is polynomial in $\cos\theta$ (which is itself rational via Weierstrass for rational parameter $t$).
\end{itemize}

No $\exp(\cdot)$, no $\sin(\cdot)$, no $\cos(\cdot)$ at the core layer. Both systems defer transcendental evaluation to the boundary.

\subsection{Phase 3: Boundary Projection}

Both systems re-enter ``hardware space'' only at the boundary:
\begin{itemize}
    \item Zhang: Grassmann subspaces are projected back to hidden state vectors via gated mixing
    \item Quadray-RT: Rotors are converted to $3 \times 3$ matrices via \texttt{toMatrix3()} only when the GPU shader requires Cartesian coordinates
\end{itemize}

This \emph{deferred materialization} principle is key: radical and transcendental expansions happen \textbf{once per boundary crossing}, not once per operation.

%==============================================================================
\section{Three Concrete Acceleration Paths}
%==============================================================================

\subsection{Path 1: Spread-Based Attention Replaces Softmax}

\subsubsection{The Problem with Softmax}

Standard attention computes $\text{softmax}(QK^\top / \sqrt{d_k})$, requiring $\exp(x_i)$ for every element. On current hardware:

\begin{itemize}
    \item $\exp(\cdot)$ requires 10--20 FLOPs (Taylor series or lookup + interpolation)
    \item Numerical stability demands the ``log-sum-exp trick'' (additional passes over data)
    \item Outputs are irrational for rational inputs---hostile to INT8/INT4 quantization
    \item Total cost: $O(n^2 \cdot d)$ transcendental evaluations per attention layer
\end{itemize}

\subsubsection{Spread as Attention Score}

The RT spread formula computes the same geometric quantity---how ``different'' two vectors are---using only algebraic operations:

\begin{equation}\label{eq:spread-attention}
s(\mathbf{q}, \mathbf{k}) = 1 - \frac{(\mathbf{q} \cdot \mathbf{k})^2}{Q(\mathbf{q}) \cdot Q(\mathbf{k})}
\end{equation}

This requires: one dot product (existing), two quadrances (existing), one multiply, one divide, one subtract. \textbf{No transcendental functions.}

\begin{observation}[Spread $\leftrightarrow$ Cosine Similarity]
The standard cosine similarity $\cos\theta = \frac{\mathbf{q} \cdot \mathbf{k}}{|\mathbf{q}||\mathbf{k}|}$ requires two square roots (for the norms). Spread avoids both:
\begin{equation}
s = 1 - \cos^2\theta = \sin^2\theta
\end{equation}
The spread is the \emph{complement of the squared cosine similarity}, computed without any radical operations.
\end{observation}

\begin{proposition}[Rational Preservation]
If query $\mathbf{q}$ and key $\mathbf{k}$ have rational (or integer) components, then $s(\mathbf{q}, \mathbf{k})$ is \textbf{exactly rational}. No precision is lost. This holds for any embedding dimension $d$.
\end{proposition}

\begin{proof}
$\mathbf{q} \cdot \mathbf{k} = \sum q_i k_i$ is rational. $Q(\mathbf{q}) = \sum q_i^2$ and $Q(\mathbf{k}) = \sum k_i^2$ are rational. The ratio of rationals is rational. $1 - r$ for rational $r$ is rational. \qed
\end{proof}

This means spread-based attention scores can be computed in \textbf{exact fixed-point arithmetic} on quantized hardware, with \textbf{zero approximation error}.

\subsubsection{Spread Normalization}

Softmax provides a probability distribution (scores sum to 1). Spread-based scores can be normalized algebraically:

\begin{equation}
\alpha_i = \frac{s(\mathbf{q}, \mathbf{k}_i)}{\sum_j s(\mathbf{q}, \mathbf{k}_j)}
\end{equation}

This is a rational function of rational values---still exact, still no transcendentals. The normalization division is a single operation per query, amortized over all keys.

Note: spread measures perpendicularity (high spread = different), which inverts the attention intuition (high score = relevant). For attention, we can use the complementary \textbf{cross} value $c = 1 - s = \cos^2\theta$, which is high when vectors are aligned:

\begin{equation}\label{eq:cross-attention}
\alpha_i = \frac{c(\mathbf{q}, \mathbf{k}_i)}{\sum_j c(\mathbf{q}, \mathbf{k}_j)} = \frac{(\mathbf{q} \cdot \mathbf{k}_i)^2 / (Q(\mathbf{q}) \cdot Q(\mathbf{k}_i))}{\sum_j (\mathbf{q} \cdot \mathbf{k}_j)^2 / (Q(\mathbf{q}) \cdot Q(\mathbf{k}_j))}
\end{equation}

When all keys have equal quadrance (e.g., after layer normalization), this simplifies further:

\begin{equation}
\alpha_i = \frac{(\mathbf{q} \cdot \mathbf{k}_i)^2}{\sum_j (\mathbf{q} \cdot \mathbf{k}_j)^2}
\end{equation}

Pure dot products, squared, normalized. No $\exp$, no $\sqrt{\cdot}$, no transcendentals anywhere.

%----------------------------------------------------------------------
\subsection{Path 2: Weierstrass Position Encoding Replaces Sinusoidal}

\subsubsection{The Problem with Sinusoidal Encoding}

Standard transformers encode position via:
\begin{align}
PE_{(pos, 2i)} &= \sin\!\left(\frac{pos}{10000^{2i/d}}\right) \\
PE_{(pos, 2i+1)} &= \cos\!\left(\frac{pos}{10000^{2i/d}}\right)
\end{align}

Each position requires $d$ transcendental function evaluations. For a sequence of length $n$ with embedding dimension $d$, that is $n \cdot d$ calls to $\sin/\cos$---evaluated at model initialization or cached, but fundamentally transcendental.

\subsubsection{Weierstrass Rational Parametrization}

The Weierstrass substitution $t = \tan(\theta/2)$ gives:
\begin{align}\label{eq:weierstrass}
\cos\theta &= \frac{1 - t^2}{1 + t^2} \\
\sin\theta &= \frac{2t}{1 + t^2}
\end{align}

\begin{theorem}[Wildberger / Weierstrass]
For any \textbf{rational} parameter $t \in \mathbb{Q}$, both $\cos\theta$ and $\sin\theta$ are \textbf{exact rationals}. No transcendental functions are evaluated.
\end{theorem}

\textbf{Proposed encoding}: Instead of $\sin(pos / 10000^{2i/d})$, define a rational parameter schedule $t_{pos,i} \in \mathbb{Q}$ and compute:
\begin{align}
PE_{(pos, 2i)} &= \frac{2 \cdot t_{pos,i}}{1 + t_{pos,i}^2} \\
PE_{(pos, 2i+1)} &= \frac{1 - t_{pos,i}^2}{1 + t_{pos,i}^2}
\end{align}

Each evaluation requires: one multiply ($t^2$), two additions, two divides. \textbf{Five arithmetic operations} versus a Taylor series.

\begin{observation}[Quantization Advantage]
If $t_{pos,i}$ is chosen from a fixed-point rational grid (e.g., multiples of $1/256$ for INT8), the position encodings are \textbf{exactly representable} in fixed-point arithmetic. Standard sinusoidal encodings introduce rounding error at every position---Weierstrass encodings introduce \textbf{none}.
\end{observation}

\subsubsection{Frequency Hierarchy via Rational Parameters}

The standard sinusoidal encoding uses exponentially decreasing frequencies ($10000^{-2i/d}$) to encode position at multiple scales. The Weierstrass encoding can achieve the same multi-scale structure by choosing parameter schedules that span different ``angular rates'':

\begin{equation}
t_{pos,i} = \frac{pos}{r^i} \quad \text{for base } r \in \mathbb{Q}
\end{equation}

For small $t$ (low-frequency dimensions), $\sin\theta \approx 2t$ and $\cos\theta \approx 1 - t^2$, so the encoding approximates the standard sinusoidal form. For large $t$ (high-frequency dimensions), the rational parametrization naturally wraps through the unit circle via a different algebraic path, providing position discrimination without periodic transcendental evaluation.

%----------------------------------------------------------------------
\subsection{Path 3: Circulant Rotor Matrices Replace Dense Attention Weights}

\subsubsection{Overparameterized Weight Matrices}

A standard attention head multiplies by dense $d \times d$ matrices $W_Q, W_K, W_V$---$3d^2$ parameters per head, $O(d^3)$ to apply. Zhang showed that geometric structure (Grassmann subspaces) can replace much of this dense computation.

Quadray-RT offers a specific algebraic structure: \textbf{circulant matrices}.

\subsubsection{F, G, H Rotor Structure}

Rotation about a Quadray basis axis uses Tom Ace's formula:
\begin{align}
F &= \frac{2\cos\theta + 1}{3} \\
G &= \frac{2\cos(\theta - 120°) + 1}{3} \\
H &= \frac{2\cos(\theta + 120°) + 1}{3}
\end{align}

The resulting $4 \times 4$ rotation matrix has \textbf{circulant} substructure:
\begin{equation}\label{eq:circulant}
R_W = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & F & H & G \\
0 & G & F & H \\
0 & H & G & F
\end{pmatrix}
\end{equation}

The $3 \times 3$ submatrix is circulant---each row is a cyclic permutation of $(F, H, G)$.

\begin{proposition}[Structured Efficiency]
A circulant $n \times n$ matrix-vector product can be computed in $O(n \log n)$ via FFT, compared to $O(n^2)$ for a general dense matrix.
\end{proposition}

For AI applications, this means a layer that applies ``tetrahedral rotation'' to token embeddings costs $O(d \log d)$ per token instead of $O(d^2)$---the same asymptotic improvement that Zhang achieves via Grassmann geometry, but through a different algebraic mechanism.

\subsubsection{Exact Rational Coefficients at Key Angles}

For algebraically significant rotation angles, the F, G, H coefficients are \textbf{exact rationals}:

\begin{center}
\begin{tabular}{ccccl}
\toprule
\textbf{Angle} & $F$ & $G$ & $H$ & \textbf{Arithmetic} \\
\midrule
$0°$ (identity) & $1$ & $0$ & $0$ & Integer \\
$120°$ & $0$ & $1/3$ & $1/3$ & Rational \\
$180°$ & $-1/3$ & $2/3$ & $2/3$ & Rational \\
$240°$ & $0$ & $1/3$ & $1/3$ & Rational (= $120°$) \\
$360°$ (identity) & $1$ & $0$ & $0$ & Integer \\
\bottomrule
\end{tabular}
\end{center}

At $120°$, the rotation matrix becomes a \textbf{pure cyclic permutation} scaled by $1/3$. At $180°$, it becomes a Janus inversion. These are among the most useful rotations in tetrahedral geometry---and they require \textbf{no floating-point arithmetic at all}.

%==============================================================================
\section{Quantization-Friendly Exactness}
%==============================================================================

The AI industry is converging on reduced-precision inference: INT8, INT4, and even binary/ternary weights. The fundamental barrier to quantization is that standard neural network operations produce irrational outputs from rational inputs:

\begin{itemize}
    \item $\exp(1) = e \approx 2.71828...$ (softmax)
    \item $\sin(\pi/6) = 0.5$ (this one is exact---but $\sin(\pi/7)$ is not)
    \item $\sqrt{x}$ (layer normalization)
\end{itemize}

Every such operation introduces rounding error when quantized. Through multiple layers, these errors compound.

\subsection{RT Algebra: Rational In, Rational Out}

The spread and cross formulas have a remarkable property:

\begin{theorem}[Rational Closure of Spread Algebra]
If all input vectors have rational (or integer) components, then:
\begin{enumerate}
    \item Quadrance $Q = \sum x_i^2$ is rational
    \item Dot product $\mathbf{u} \cdot \mathbf{v} = \sum u_i v_i$ is rational
    \item Spread $s = 1 - (\mathbf{u} \cdot \mathbf{v})^2 / (Q_u \cdot Q_v)$ is rational
    \item Cross $c = 1 - s$ is rational
    \item Weierstrass parametrization $(1-t^2)/(1+t^2)$ and $2t/(1+t^2)$ are rational for rational $t$
\end{enumerate}
The entire algebra is \textbf{closed over $\mathbb{Q}$}.
\end{theorem}

This means a neural network layer built from spread/cross operations and Weierstrass-parametrized rotations can be computed in \textbf{exact fixed-point arithmetic} with \textbf{zero quantization error} at the algebraic core. The only precision loss occurs at boundary conversions (e.g., to floating-point for GPU shaders).

\subsection{Comparison: Error Propagation Through Layers}

Consider a network with $L$ layers, each applying a transformation $T_\ell$:

\begin{center}
\begin{tabular}{lcc}
\toprule
\textbf{Operation Type} & \textbf{Error per Layer} & \textbf{Error after $L$ Layers} \\
\midrule
Transcendental (exp, sin) & $\epsilon_{\text{quant}}$ & $O(L \cdot \epsilon_{\text{quant}})$ \\
Algebraic (spread, cross) & $0$ (exact) & $0$ (exact) \\
Mixed (algebraic core + boundary) & $0$ core, $\epsilon$ boundary & $\epsilon$ (constant) \\
\bottomrule
\end{tabular}
\end{center}

With RT algebra, quantization error does not \emph{accumulate through layers}---it occurs only at the boundary where the algebraic domain meets the hardware representation. This is the difference between $O(L)$ and $O(1)$ error growth.

\subsection{Empirical Precedent: Path C Exact Arithmetic}

This is not merely theoretical. The ARTexplorer project's Prime Projection Conjecture~[6] required solving a concrete instance of the same problem: determining exact convex hull vertex counts from projected polyhedra, where floating-point ambiguity in cross products ($\sim 10^{-17}$) could flip a 7-gon to a 6-gon.

The solution---\textbf{Path C exact arithmetic}---converts IEEE 754 Float64 coordinates to Python's \texttt{fractions.Fraction} (arbitrary-precision rationals), then computes cross products with exact integer arithmetic:

\begin{verbatim}
from fractions import Fraction
frac_pts = [(Fraction(p[0]), Fraction(p[1])) for p in projected_2d]
cross = (a[0]-o[0])*(b[1]-o[1]) - (a[1]-o[1])*(b[0]-o[0])
# cross is a Fraction -- exactly zero or not. No epsilon needed.
\end{verbatim}

This eliminated \emph{all} floating-point ambiguity from hull counting. The geometric discovery---that non-constructible prime polygons (7, 11, 13-gon) emerge as projections of geodesic tetrahedra at rational spreads---was only possible because the arithmetic was exact.

The parallel to AI inference is direct: when a neural network's internal computation is algebraically exact, the \emph{decisions} it makes (attention routing, classification boundaries) are deterministic. Floating-point non-determinism in $\exp(\cdot)$ evaluation is a known source of inference irreproducibility across hardware platforms. Spread-based attention would eliminate this entirely.

\subsection{Tiered Rational Parameter Spaces}

The Prime Projection search~[6] also demonstrates a methodology directly applicable to neural architecture: \textbf{tiered rational parameter search}. Instead of sweeping a decimal grid ($101^3 \approx 10^6$ configurations), the search was organized by algebraic significance:

\begin{center}
\begin{tabular}{clcl}
\toprule
\textbf{Tier} & \textbf{Denominators} & \textbf{Spread Values} & \textbf{Radical Family} \\
\midrule
1 (RT-pure) & $\{2, 3, 4\}$ & 7 values, 343 configs & $\sqrt{2}, \sqrt{3}$ \\
2 ($\varphi$-family) & $\{5, 8, 10\}$ & 19 values & $\sqrt{5}$ \\
3 (algebraic) & $\{6, 9, 12, 16, 20, 25\}$ & 67 values & mixed \\
\bottomrule
\end{tabular}
\end{center}

The key result: \textbf{all four prime polygons (5, 7, 11, 13) were found at Tier~1}---the simplest rational spreads with denominators $\{2, 3, 4\}$. The algebraically simplest parameters produced the most significant geometric structures.

\begin{observation}[Tier Hierarchy for Neural Architecture Search]
In quantized neural networks, weights are restricted to discrete values (e.g., $\{-1, 0, 1\}$ for ternary, $\{0, 1/4, 1/2, 3/4, 1\}$ for 2-bit). These are \emph{exactly} Tier~1 rational spreads. The Prime Projection result suggests that the most geometrically significant transformations---the ones that produce structurally rich outputs---naturally live at the simplest rational values. This would explain why aggressive quantization (even to ternary weights) preserves model quality better than precision analysis alone predicts: the important structure is already rational.
\end{observation}

%==============================================================================
\section{Tetrahedral Topology Matches Multi-Head Attention}
%==============================================================================

\subsection{Four Basis Vectors, Four Attention Heads}

Standard transformers use multiple attention heads (typically 4, 8, 12, or 16) to capture different ``directions'' in meaning space. The Quadray system provides exactly 4 basis vectors $(\hat{W}, \hat{X}, \hat{Y}, \hat{Z})$, each maximally spread from the others:

\begin{equation}
s(\hat{W}, \hat{X}) = s(\hat{W}, \hat{Y}) = s(\hat{X}, \hat{Z}) = \ldots = \frac{8}{9}
\end{equation}

This is the \textbf{maximum achievable mutual spread} for 4 vectors in 3-space (tetrahedral packing). Each Quadray axis ``looks'' in a maximally different direction---precisely what multi-head attention is designed to achieve.

\begin{observation}[Natural 4-Head Architecture]
A Quadray-native attention mechanism with one head per basis axis would have:
\begin{itemize}
    \item 4 heads with \textbf{guaranteed maximal diversity} (spread $8/9$)
    \item Circulant weight structure within each head (3 parameters instead of $d^2$)
    \item Symmetric treatment of non-self dimensions (each head attends equally to the other three)
    \item Exact rational scores for aligned/orthogonal relationships
\end{itemize}
\end{observation}

For 8- or 16-head architectures, nested tetrahedral structures (e.g., the compound of two tetrahedra forming a cube, or the compound of five tetrahedra forming a dodecahedron) provide natural multi-scale head arrangements with algebraically exact mutual spreads.

\subsection{Symmetry Breaking and the Central Symmetry Barrier}

The Prime Projection Conjecture~[6] established a theorem with direct implications for attention architecture:

\begin{theorem}[Central Symmetry Barrier]
If a polyhedron has central (inversion) symmetry, ALL orthographic projections have even convex hull vertex counts. Proof: for every vertex $v$ on the hull, its antipodal partner $-v$ either also lies on the hull or is excluded symmetrically. Hull vertices come in pairs $\Rightarrow$ even count.
\end{theorem}

Only \textbf{asymmetric} polyhedra---those lacking central inversion---can produce odd (and specifically prime) hull counts. The truncated tetrahedron and geodesic tetrahedra are asymmetric; the cuboctahedron and octahedron are not.

\begin{observation}[Symmetry Breaking in Attention]
Standard multi-head attention treats all heads symmetrically (same architecture, different learned weights). The Central Symmetry Barrier suggests this is a structural limitation: symmetric architectures can only produce ``even'' (symmetric) attention patterns. To access richer structure---the geometric analogue of prime polygons---\textbf{deliberate asymmetry} is needed.

Quadray coordinates provide this naturally: the four tetrahedral axes have two chirality classes (W/Y are right-circulant, X/Z are left-circulant in the rotor framework~[5]). A 4-head Quadray attention mechanism would have 2 right-handed and 2 left-handed heads---built-in chirality breaking that symmetric architectures must learn.
\end{observation}

\subsection{The Cartesian Blind Spot}

The Janus Inversion framework~[10] identifies a cognitive property of coordinate systems relevant to AI architecture design: \textbf{the coordinate system you choose determines which structures you can efficiently represent, and which questions you naturally ask}.

In Cartesian coordinates, all of 3D space requires both positive and negative values. The question ``what is negative space?'' is trivial---it is the opposite octant.

In Quadray coordinates, all of 3D space is already covered by positive values alone ($w, x, y, z \geq 0$). The question ``what does the negative region mean?'' becomes non-trivial and geometrically productive: it maps the tetrahedron to its dual (vertices $\leftrightarrow$ face-centers), revealing structure that Cartesian coordinates structurally obscure.

For AI: embedding spaces are conventionally Cartesian (orthogonal basis). The choice is so automatic it is invisible. But if the tetrahedron is the minimum-complexity structure that maximizes representational diversity (Section~8.2), then Cartesian embeddings may be hiding structure that tetrahedral embeddings would reveal---just as Cartesian coordinates hide the tetrahedron-dual relationship that Quadray makes visible.

\subsection{The Fourth Coordinate as Shape Information}

The Janus Inversion analysis~[10] demonstrates that the ``redundant'' 4th Quadray parameter carries real geometric content. When the zero-sum constraint ($w + x + y + z = k$) is enforced, 4D collapses to 3D---isomorphic to Cartesian, with identical expressive power. When the constraint is released, the 4th parameter encodes \textbf{shape} information beyond position: a deformed tetrahedron with weights $(1, 1, 1, 6)$ occupies the same position as $(1, 1, 1, 1)$ but encodes a different geometric relationship.

In embedding space, this suggests that 4D tetrahedral embeddings (without zero-sum constraint) could carry one additional degree of freedom per token---encoding not just ``where'' a token sits in meaning space, but ``what shape'' its meaning takes. This is analogous to the distinction between a word's denotation (position) and its connotation (shape).

\subsection{The Janus Polarity Bit and Semantic Valence}

In the Spread-Quadray Rotor framework, an explicit $\mathbb{Z}_2$ polarity flag distinguishes between $+$ and $-$ orientations. In semantic embedding space, this has a natural interpretation:

\begin{itemize}
    \item \textbf{High cosine similarity, same polarity}: synonyms (``hot'' / ``warm'')
    \item \textbf{High cosine similarity, opposite polarity}: antonyms (``hot'' / ``cold'')
    \item \textbf{Low cosine similarity}: unrelated (``hot'' / ``bicycle'')
\end{itemize}

Standard attention mechanisms conflate synonyms and antonyms (both produce high $\cos\theta$). Zhang's Grassmann manifold, being a quotient space, also identifies opposite orientations. The Janus polarity bit makes this distinction \textbf{explicit}:

\begin{equation}
\text{Quadray Rotor} = (W, X, Y, Z, \pm) \in \mathbb{R}^4 \times \mathbb{Z}_2
\end{equation}

This suggests that a polarity-aware attention mechanism could naturally distinguish semantic valence without requiring additional learned parameters.

%==============================================================================
\section{The Deferred Materialization Principle}
%==============================================================================

Both Zhang's Grassmann architecture and Quadray-RT follow what we term \textbf{deferred materialization}:

\begin{tcolorbox}[colback=green!5!white, colframe=green!50!black, title=Deferred Materialization]
Work in the algebraically exact representation for as long as possible. Only convert to hardware-native format (floating-point, pixel coordinates, attention weights) at the \textbf{boundary} where the computation meets the physical device.
\end{tcolorbox}

In Quadray-RT, this is implemented concretely:

\begin{enumerate}
    \item \textbf{PurePhi}: Golden ratio algebra in symbolic $(a + b\sqrt{5})/c$ form, expanded to decimal only at \texttt{toDecimal()} call
    \item \textbf{PureRadicals}: $\sqrt{2}$, $\sqrt{3}$, $\sqrt{6}$ cached as IEEE 754 doubles, expanded once
    \item \textbf{PureCubics}: Non-constructible polygon constants (heptagon, nonagon) solved once, cached
    \item \textbf{Spread algebra}: All rotation operations in spread/cross space, $\sqrt{\cdot}$ extracted only for matrix output
\end{enumerate}

The analogous principle for AI inference:

\begin{enumerate}
    \item \textbf{Embedding layer}: Integer token IDs $\to$ rational/integer embedding vectors
    \item \textbf{Attention layers}: Spread/cross algebra (exact rational throughout)
    \item \textbf{Position encoding}: Weierstrass parametrization (exact rational for rational $t$)
    \item \textbf{Output layer}: Project to vocabulary logits---only here does precision loss occur
\end{enumerate}

The prediction: a transformer-equivalent architecture with spread-based attention and Weierstrass position encoding would require \textbf{one boundary conversion} (at the output softmax) instead of boundary conversions at \textbf{every layer}.

%==============================================================================
\section{Computational Cost Comparison}
%==============================================================================

\begin{center}
\begin{tabular}{p{3.2cm}p{4cm}p{4cm}p{2cm}}
\toprule
\textbf{Operation} & \textbf{Standard Transformer} & \textbf{Quadray-RT Equivalent} & \textbf{Savings} \\
\midrule
Attention scores & softmax($\exp(\cdot)$)---transcendental & Spread $s = 1 - \text{dot}^2/(Q_1 Q_2)$---algebraic & $\sim$10$\times$ fewer FLOPs per score \\
\addlinespace
Position encoding & $\sin/\cos$ series---transcendental & Weierstrass $(1-t^2)/(1+t^2)$---rational & Exact in INT arithmetic \\
\addlinespace
Rotation / transform & Dense matrix multiply $O(d^2)$ & Circulant F,G,H multiply $O(d \log d)$ & Structured sparsity \\
\addlinespace
Quantization error & Accumulates $O(L)$ through layers & Zero for rational spreads, $O(1)$ at boundary & Higher accuracy at lower precision \\
\addlinespace
Normalization & LayerNorm ($\sqrt{\cdot}$, division) & Quadrance $Q = \sum x_i^2$ (no $\sqrt{\cdot}$) & Defer $\sqrt{\cdot}$ to boundary \\
\bottomrule
\end{tabular}
\end{center}

%==============================================================================
\section{Discussion: ``LLM = Geometry''}
%==============================================================================

\subsection{The Convergence is Real}

Zhang's paper and Quadray-RT arrive at the same conclusion from opposite directions:

\begin{itemize}
    \item \textbf{Zhang} (top-down): ``Neural network computation \emph{is} geometric transformation. Let's use the right geometry.'' Starting from AI, discovers the geometry.
    \item \textbf{Quadray-RT} (bottom-up): ``Geometric computation should use the right algebra. Let's eliminate transcendentals.'' Starting from geometry, discovers the computation.
\end{itemize}

The meeting point is the claim that \textbf{algebraic geometry---polynomial and rational operations on structured manifolds---is both sufficient and optimal for the transformations that neural networks perform}.

\subsection{Why Tetrahedral Geometry Specifically?}

The tetrahedron is the simplest 3D polyhedron (4 vertices, 6 edges, 4 faces). Fuller called it the ``minimum system'' of Universe---the simplest structure that encloses space. This minimality has algebraic consequences:

\begin{enumerate}
    \item \textbf{Maximum spread diversity}: 4 vectors with mutual spread $8/9$ provide the most ``different'' set of directions achievable in 3-space
    \item \textbf{Circulant structure}: Tetrahedral symmetry produces circulant submatrices, enabling FFT acceleration
    \item \textbf{Rational basis spread}: $8/9$ is exact, unlike the orthogonal basis spread of $1$ which, while also exact, leads to coordinate systems requiring $\sqrt{2}$, $\sqrt{3}$ for tetrahedral geometry
    \item \textbf{Natural dimension}: 4 basis vectors for 3D space provides exactly one redundant degree of freedom---enough to escape $SO(3)$ singularities without the normalization constraint of quaternions
\end{enumerate}

In AI terms: the tetrahedron provides the minimum-complexity structure that achieves maximum representational diversity with exact arithmetic.

\subsection{Open Questions}

\begin{enumerate}
    \item \textbf{Empirical validation}: Does spread-based attention actually match softmax attention in language modelling benchmarks? Zhang showed Grassmann layers are competitive---spread attention is algebraically simpler, but the question is empirical.

    \item \textbf{Gradient flow}: Can spread/cross operations be differentiated efficiently for backpropagation? The operations are all rational functions, so gradients are also rational---but numerical stability of gradient computation needs investigation.

    \item \textbf{Expressiveness}: Softmax attention can represent any probability distribution over keys. Can cross-normalized attention (Equation~\ref{eq:cross-attention}) match this expressiveness? The squared dot product is always non-negative, which is good for attention weights, but the distribution shape differs from softmax.

    \item \textbf{Hardware support}: Current AI accelerators (GPUs, TPUs) are optimized for dense matrix multiplication and transcendental functions. Would spread-based architectures require new hardware, or can they leverage existing INT8/INT4 pipelines more efficiently?

    \item \textbf{Scaling laws}: Do the precision advantages of exact rational arithmetic compound at scale? A model with $O(1)$ quantization error instead of $O(L)$ could potentially be trained with lower precision throughout, reducing memory and communication costs.
\end{enumerate}

%==============================================================================
\section{Anticipated Objections}
%==============================================================================

We address here several substantive criticisms that would arise in peer review of both Zhang's original work and our proposed extensions. These objections are serious and deserve serious responses.

\subsection{Objection 1: ``Zhang chose a handicapped baseline''}

\begin{tcolorbox}[colback=red!5!white, colframe=red!50!black, title=The Objection]
It is well known that transformers are wasteful and inaccurate at small-scale tasks where decision trees, CNNs, and structured models perform better. At 13--18M parameters, the comparison baseline is handicapped---transformers only demonstrate their advantage at scale. By showing competitive results at small scale, the author has not demonstrated improvement over existing techniques.
\end{tcolorbox}

\textbf{Response.} This objection is factually correct about scale dynamics and must be taken seriously. Transformers' quadratic attention cost is \emph{justified} at scale precisely because the $O(n^2)$ global context window enables emergent capabilities (in-context learning, chain-of-thought reasoning) that local methods cannot replicate. At 13--18M parameters, a well-tuned CNN or LSTM will often match or exceed a transformer on perplexity benchmarks.

However, the objection misidentifies the claim. Zhang's result is not ``we beat transformers.'' It is: \textbf{geometric operations on manifolds can replicate transformer-level performance without computing the full attention matrix}. The significance is architectural, not numerical. If Grassmann layers match attention at 18M parameters, the question becomes whether they \emph{also} match at 3B and 7B---where attention's quadratic cost becomes the dominant bottleneck.

For the Quadray-RT extension, the scale question is even more pointed: does exact rational arithmetic \emph{compound} its advantage at scale, or plateau? We predict the former (Section~5.2, $O(1)$ vs.\ $O(L)$ error growth), but this is an empirical claim that requires validation at scale. \textbf{The honest answer is: we don't yet know.}

\subsection{Objection 2: ``Show me 3B and 7B results''}

\begin{tcolorbox}[colback=red!5!white, colframe=red!50!black, title=The Objection]
Results at 3B--7B parameters would demonstrate whether geometric methods maintain competitive performance at scales where attention genuinely matters but training remains tractable. Even better: distill a known-good LLM into a geometric architecture and measure what is preserved.
\end{tcolorbox}

\textbf{Response.} This is the right experiment. We agree completely that the research programme outlined here is incomplete without mid-scale validation. The specific experiments needed are:

\begin{enumerate}
    \item \textbf{Geometric distillation}: Take a known-good 7B transformer (e.g., Llama-3 8B or Mistral 7B), replace attention layers with spread-based or Grassmann layers, and distill. Measure: (a) what fraction of benchmark performance is preserved, (b) inference latency reduction, (c) quantization tolerance.

    \item \textbf{Hybrid architecture}: Replace only the \emph{middle} layers with geometric attention (where the representation is most abstract), keeping standard attention at input/output boundaries. This tests whether geometric layers can serve as efficient ``backbone'' computation while attention handles boundary conditions.

    \item \textbf{INT4 stress test}: Train a spread-based model and a standard transformer at identical scale, then quantize both to INT4. The prediction from Section~5.2 is that the spread-based model degrades less, because its core operations are already rational. If this prediction fails, the quantization argument collapses.
\end{enumerate}

Until these experiments are run, this paper's claims about scale remain theoretical. We are explicit about this limitation.

\subsection{Objection 3: ``The real claim is architectural, not accuracy''}

\begin{tcolorbox}[colback=yellow!5!white, colframe=yellow!50!black, title=The Nuance]
The performance gains in Zhang are modest. The interesting result is not ``+0.X\% accuracy'' but that Grassmann layers match transformer-level results \emph{without attention at all}. Architecturally, that's the real claim---the full $n \times n$ attention matrix is not necessary for competitive sequence modelling.
\end{tcolorbox}

\textbf{Response.} We endorse this reading. The contribution of geometric methods to AI is not (yet) about beating SOTA benchmarks. It is about demonstrating that the \textbf{core computation} of sequence modelling---capturing token-to-token relationships---can be performed by structured geometric operations rather than unstructured dense matrix exponentiation.

This matters for three reasons that transcend benchmark scores:

\begin{enumerate}
    \item \textbf{Interpretability}: Geometric operations (rotation, projection, subspace deformation) have meaning. A spread-based attention score of $s = 3/4$ means ``these vectors are at 60° spread''---a geometric relationship that can be visualized and reasoned about. A softmax score of 0.73 means nothing without context.

    \item \textbf{Hardware co-design}: If the core operations are algebraic (polynomial/rational), hardware can be specialized. Circulant matrix units, fixed-point spread accumulators, and Weierstrass function units are simpler circuits than transcendental function approximators. The architectural claim enables hardware claims.

    \item \textbf{Theoretical understanding}: If ``LLM = Geometry'' is correct---if the transformations learned by neural networks are fundamentally geometric---then understanding which geometry (Grassmann, tetrahedral, hyperbolic, etc.) best matches which task class is a more productive research direction than scaling up a single architecture.
\end{enumerate}

\subsection{Objection 4: ``Attention alternatives already exist---RWKV, Mamba, etc.''}

\begin{tcolorbox}[colback=red!5!white, colframe=red!50!black, title=The Objection]
We already know attention is not the only possible mechanism. RWKV~[11] achieves transformer-competitive performance with linear-time recurrence. Mamba~[12] uses selective state spaces. RetNet uses retention mechanisms. Zhang's Grassmann approach is one more entry in a growing list of attention alternatives. What does Quadray-RT add beyond yet another alternative?
\end{tcolorbox}

\textbf{Response.} This objection correctly situates the work within a broader trend. RWKV, Mamba, RetNet, and Grassmann layers are all demonstrations that $O(n^2)$ attention is sufficient but not necessary. Our contribution is not another attention alternative---it is a \textbf{mathematical toolkit} that could improve \emph{any} of them.

The key distinction:

\begin{center}
\begin{tabular}{lll}
\toprule
\textbf{System} & \textbf{Contribution} & \textbf{Core Operations} \\
\midrule
RWKV & Architecture (linear RNN) & Standard floating-point \\
Mamba & Architecture (selective SSM) & Standard floating-point \\
Grassmann & Architecture (manifold layers) & Pl\"ucker coordinates \\
\textbf{Quadray-RT} & \textbf{Algebra} (exact rational) & \textbf{Spread/cross, Weierstrass} \\
\bottomrule
\end{tabular}
\end{center}

RWKV, Mamba, and Grassmann layers all still compute with IEEE 754 floating-point arithmetic, including transcendental functions where needed. Quadray-RT proposes that \emph{whatever architecture you choose}, its core operations can be reformulated in exact rational algebra---spread instead of cosine, Weierstrass instead of sinusoidal, circulant instead of dense.

\begin{observation}[Algebra is orthogonal to architecture]
Spread-based scoring could be applied within RWKV's recurrence, Mamba's state-space updates, or Grassmann's subspace deformations. The algebraic toolkit is \textbf{composable with any architecture} that performs geometric operations on token representations. This is not a competing alternative to these methods---it is a potential \emph{acceleration layer} beneath them.
\end{observation}

A concrete test: implement RWKV's channel mixing with spread-normalized scores instead of softmax, quantize to INT4, and compare quality degradation against standard RWKV-INT4. If the spread version degrades less, the algebraic contribution is validated independent of architecture choice.

\subsection{Objection 5: ``Squared dot products lose sign information''}

\begin{tcolorbox}[colback=red!5!white, colframe=red!50!black, title=The Objection]
Cross-normalized attention (Equation~\ref{eq:cross-attention}) uses $(\mathbf{q} \cdot \mathbf{k})^2$, which is always non-negative. Standard attention uses $\mathbf{q} \cdot \mathbf{k}$, which can be negative (softmax then suppresses negative scores exponentially). Squaring discards sign information---anti-correlated tokens (negative dot product) receive the same weight as correlated ones.
\end{tcolorbox}

\textbf{Response.} This is a genuine technical limitation. Spread and cross are inherently unsigned measures ($s = \sin^2\theta$ cannot distinguish $\theta$ from $180° - \theta$). The Janus Inversion framework~[10] addresses this explicitly: the $\mathbb{Z}_2$ polarity bit restores sign information.

In practice, a spread-based attention mechanism would need to track polarity:
\begin{equation}
\alpha_i = \frac{\text{sgn}(\mathbf{q} \cdot \mathbf{k}_i) \cdot c(\mathbf{q}, \mathbf{k}_i)}{\sum_j |c(\mathbf{q}, \mathbf{k}_j)|}
\end{equation}

where $\text{sgn}(\cdot)$ is the sign function (a single comparison, not a transcendental). This restores signed attention weights while keeping the core computation algebraic. The Janus polarity bit provides the mathematical framework for this sign tracking.

Alternatively, for tasks where anti-correlation should produce low (not high) attention, the unsigned spread itself is appropriate: anti-correlated tokens have low spread (they are ``aligned'' in the spread sense, just pointing opposite ways). Whether this is a bug or a feature depends on the task.

%==============================================================================
\section{Conclusion}
%==============================================================================

The convergence between geometric AI (Zhang, 2025) and algebraic geometric computation (Quadray-RT) points to a concrete research programme: \textbf{replace transcendental operations in neural networks with algebraically exact equivalents, exploit tetrahedral symmetry for structured sparsity, and defer precision loss to the hardware boundary}.

The pieces exist:
\begin{itemize}
    \item Spread/cross algebra provides exact rational attention scores
    \item Weierstrass parametrization provides exact rational position encodings
    \item Circulant rotor matrices provide $O(d \log d)$ structured transforms
    \item Janus polarity provides explicit semantic valence distinction
    \item The tetrahedral basis provides maximum diversity in minimum dimensions
\end{itemize}

What remains is to assemble them into a working architecture and test empirically. If spread-based attention proves competitive with softmax, the computational savings---exact integer arithmetic, no transcendental evaluation, structured sparsity, $O(1)$ quantization error---could be substantial, particularly for edge inference where every FLOP and every bit of precision matter.

A companion benchmark workplan~[13] defines 13 tests across four cost tiers---from 5-minute CPU microbenchmarks (raw spread-vs-softmax FLOP counts, INT8 fidelity) through single-GPU component swaps (attention replacement in pre-trained GPT-2, INT4 stress tests) to multi-GPU distillation experiments. Crucially, the claims in this paper can be tested \emph{cheaply}: the make-or-break experiment (does spread-based attention degrade less under INT4 quantization than softmax?) requires only a pre-trained 124M-parameter model and approximately three hours on a single GPU. No billion-parameter training run is needed to validate or falsify the core algebraic argument. The workplan includes explicit success criteria and falsification conditions---if INT4 quantization error is equivalent or worse for spread-based scoring, the strongest claim in this paper collapses.

Zhang proved the architecture works. Wildberger provided the algebra. Fuller provided the coordinates. The question now is engineering.

%==============================================================================
% References
%==============================================================================
\section*{References}

\begin{enumerate}[label={[\arabic*]}]
    \item Zhang, C. (2025). ``Attention Is Not What You Need.'' \textit{arXiv:2512.19428}. \url{https://arxiv.org/abs/2512.19428}

    \item Wildberger, N.J. (2005). \textit{Divine Proportions: Rational Trigonometry to Universal Geometry}. Wild Egg Books.

    \item Fuller, R.B. (1975). \textit{Synergetics: Explorations in the Geometry of Thinking}. Macmillan.

    \item Urner, K. (2003). ``Quadray Coordinates.'' \url{https://www.grunch.net/synergetics/quadrays.html}

    \item Thomson, A. (2026). ``Spread-Quadray Rotors v2.0: A Tetrahedral Alternative to Quaternions for Gimbal-Lock-Free Rotation Representation.'' ARTexplorer Project.

    \item Thomson, A. (2026). ``Prime Projection Conjecture: Non-Constructible Polygons from Polyhedral Shadows.'' ARTexplorer Project.

    \item Vaswani, A. et al. (2017). ``Attention Is All You Need.'' \textit{Advances in Neural Information Processing Systems}, 30.

    \item Brouwer, L.E.J. (1912). ``\"Uber Abbildung von Mannigfaltigkeiten.'' \textit{Mathematische Annalen}, 71(4), 97--115.

    \item Ace, T. ``Rotating in Encyclopedic Space.'' \url{http://www.tomacevedo.com}

    \item Thomson, A. (2026). ``Janus Inversion: Coordinate Inversion Through the Origin in Tetrahedral Geometry.'' ARTexplorer Project, v9.

    \item Peng, B. et al. (2023). ``RWKV: Reinventing RNNs for the Transformer Era.'' \textit{arXiv:2305.13048}.

    \item Gu, A. and Dao, T. (2023). ``Mamba: Linear-Time Sequence Modeling with Selective State Spaces.'' \textit{arXiv:2312.00752}.

    \item Thomson, A. (2026). ``Accelerant Test Cases: Tiered Benchmarks for Spread-Based AI Claims.'' ARTexplorer Project. Available at \texttt{Geometry Documents/Accelerant.md} in the project repository.
\end{enumerate}

\end{document}
